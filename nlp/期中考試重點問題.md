## 📘 NLP Week 1 簡答題模擬題庫＋標準答案
### **1️⃣ 什麼是自然語言處理（NLP）？它在人工智慧中扮演什麼角色？**

**標準答案：**  
自然語言處理是人工智慧中的一個應用領域，目的是讓機器能夠理解、產生、分析、轉譯人類語言。它融合語言學、計算機科學與機器學習技術，是人機互動的關鍵技術之一。

---

### **2️⃣ NLP 常見的應用場景有哪些？**

**標準答案：**

1. **聊天機器人與語音助理**（如 Siri、Google Assistant）
    
2. **機器翻譯**（Google Translate）
    
3. **情感分析**（判斷留言是正面或負面）
    
4. **自動摘要**（如新聞摘要）
    
5. **文件分類**（垃圾郵件過濾、主題分類）
    
6. **語音辨識與輸出**
    

---

### **3️⃣ 傳統的詞表示方法有哪些？各自有什麼問題？**

**標準答案：**

1. **One-hot encoding**：每個詞為一個高維稀疏向量，缺點是向量之間無語意關聯、佔空間大。
    
2. **Bag-of-Words**：忽略詞序，僅看詞頻，無法表示語境或詞間語意。
    
3. **TF-IDF**：在 Bag-of-Words 上加入詞的重要性，但仍無法反映語意相似性。
    

---

### **4️⃣ 為什麼我們需要詞嵌入（word embedding）技術？**

**標準答案：**  
詞嵌入可將詞轉為低維、稠密的向量，並能保留語意資訊，使語意相近的詞在空間中距離接近。這種方式克服了 One-hot 編碼與 Bag-of-Words 的語意缺失問題，並可被深度學習模型直接使用。

---

### **5️⃣ 什麼是語言模型（Language Model）？其用途是什麼？**

**標準答案：**  
語言模型是一種根據前文（上下文）來預測下一個詞出現機率的模型。用途包括：

- 自動文字生成
    
- 機器翻譯
    
- 語音辨識
    
- 拼字校正  
    語言模型是 NLP 任務的基礎建構模塊。
    

---

### **6️⃣ 說明統計語言模型的基本概念及缺點。**

**標準答案：**  
統計語言模型（如 n-gram）根據大量語料中詞彙出現的機率來建立模型。例如，bigram 模型僅考慮前一個詞來預測下一個詞。  
缺點：

- 無法處理長距離依賴
    
- 遇到新詞（unseen n-gram）時無法處理
    
- 模型需要大量記憶體儲存所有 n-gram 統計
    

---

### **7️⃣ 什麼是 Distributional Hypothesis？它與詞嵌入有何關聯？**

**標準答案：**  
Distributional Hypothesis 指的是「語意相近的詞，常出現在相似的語境中」，由 Firth 在 1957 年提出。這一概念是詞嵌入方法（如 Word2Vec）背後的理論基礎，用來透過上下文學習詞語間的語意關係。


---

## 📘 NLP Week 2 簡答題模擬題庫＋標準答案

---

### **1️⃣ 什麼是 One-hot Encoding？其優缺點為何？**

**標準答案：**  
One-hot Encoding 是最基本的詞表示法，每個詞對應一個維度，詞向量中僅有一個位置為 1，其餘為 0。  
優點：簡單、無需訓練  
缺點：向量維度高且稀疏、無語意關聯、記憶體浪費

---

### **2️⃣ 什麼是 Co-occurrence Matrix（共現矩陣）？如何建構？**

**標準答案：**  
共現矩陣是用來統計詞與詞在語料中同時出現的次數。  
建構方式：

1. 建立語料庫
    
2. 設定「上下文視窗」大小（例如前後各一詞）
    
3. 建立矩陣，行為 target 詞，列為 context 詞，填入共現次數
    

---

### **3️⃣ 共現矩陣有什麼問題？如何改善？**

**標準答案：**  
問題：

- 高頻詞（如 the、is）會影響結果
    
- 無法衡量詞間語意關係
    
- 矩陣仍為高維稀疏
    

改善方法：

- 去除 stopwords
    
- 使用 Pointwise Mutual Information（PMI）進行轉換
    
- 使用 PPMI 保留正相關語意關係
    

---

### **4️⃣ 什麼是 PMI（Pointwise Mutual Information）？其數學公式為何？**

**標準答案：**  
PMI 衡量兩個詞同時出現的機率與各自獨立出現機率的比值，用來表示語意關聯性。  
數學公式：

PMI(w1,w2)=log⁡P(w1,w2)P(w1)⋅P(w2)PMI(w_1, w_2) = \log \frac{P(w_1, w_2)}{P(w_1) \cdot P(w_2)}

值越高，代表這兩個詞更常一起出現。

---

### **5️⃣ 為什麼需要 PPMI？與 PMI 有何不同？**

**標準答案：**  
PMI 的值可能為負數，代表兩詞關聯性低，但實際應用中，我們常關注正相關的詞對。  
PPMI（Positive PMI）透過取 max⁡(0,PMI)\max(0, PMI) 過濾掉負值，讓表示更穩定、適合用於詞向量建構。

---

### **6️⃣ 什麼是 TF-IDF？用在哪裡？有什麼意義？**

**標準答案：**  
TF-IDF 結合「詞頻」（TF）與「逆文檔頻率」（IDF）來衡量某詞對一篇文件的重要性。  
公式：

TF-IDF=TF(t,d)×log⁡(NDF(t))TF\text{-}IDF = TF(t, d) \times \log \left( \frac{N}{DF(t)} \right)

應用：資訊檢索、文本分類、關鍵詞提取。  
意義：可突顯具代表性的詞彙，降低高頻但無意義的詞（如「的」、「是」）影響。

---

### **7️⃣ 什麼是 LSA（Latent Semantic Analysis）？與共現矩陣有何關聯？**

**標準答案：**  
LSA 是一種降維技術，用來從詞與文件的共現矩陣中抽取語意結構。  
透過奇異值分解（SVD）對詞-文件矩陣進行分解，將高維資料映射到低維語意空間，能挖掘潛在的語意關聯。

---

### **8️⃣ 簡述詞向量從「計數式」到「學習式」的演變意義。**

**標準答案：**

- **計數式方法**：如共現矩陣、TF-IDF、PMI，依賴統計計算，結構簡單但語意表達力有限
    
- **學習式方法**：如 Word2Vec、GloVe，透過神經網路從上下文中學習向量表示，能捕捉語意關聯與類比關係  
    演變的意義是：模型逐漸由「統計記憶」轉向「語意理解」
    

---
噢～你真的來了。**Week 3：詞嵌入地獄主題週**，歡迎來到 NLP 的重訓中心。在這週，我們正式丟掉統計表格，走進神經網路的溫柔陷阱。  
準備好背 `softmax`、`hierarchical softmax`、`negative sampling` 的定義，然後懷疑人生吧。

---

## 📘 NLP Week 3 簡答題模擬題庫＋標準答案

---

### **1️⃣ Word2Vec 是什麼？其目的是什麼？**

**標準答案：**  
Word2Vec 是一種由 Google Mikolov 團隊提出的詞嵌入模型，用來將詞語轉換為低維向量，並保留語意關係。  
其目的是透過上下文學習詞的分布式語意表示，使語意相近的詞在向量空間中距離更接近。

---

### **2️⃣ CBOW 與 Skip-gram 的架構差異是什麼？**

**標準答案：**

- **CBOW（Continuous Bag of Words）**：使用上下文詞來預測中間詞。
    
- **Skip-gram**：使用中間詞來預測上下文。
    

CBOW 在大資料下較快訓練，Skip-gram 在處理小樣本及稀有詞彙時效果較佳。

---

### **3️⃣ Word2Vec 為什麼要使用 softmax？有什麼問題？**

**標準答案：**  
softmax 用來將輸出層轉換為機率分佈，讓模型可以針對詞彙表中每個詞計算預測機率。  
問題在於當詞彙表非常大（例如十萬以上），softmax 的計算成本極高，因為需要對每個詞做一次計算與正規化。

---

### **4️⃣ 為什麼要取 log 機率來做訓練？**

**標準答案：**

- 使數值範圍壓縮，避免機器學習過程中出現極端值造成不穩定。
    
- 對極小機率也能保留差異性
    
- 更適合用於最大化機率乘積的目標函數（等價於最小化負 log likelihood）
    

---

### **5️⃣ 試解釋 Hierarchical Softmax 是什麼？為什麼能加速訓練？**

**標準答案：**  
Hierarchical Softmax 使用 Huffman Tree 來組織詞彙，每個詞對應到樹上一條路徑，只需計算該路徑上的節點即可。  
這將 softmax 的複雜度從 O(V) 降到 O(log V)，顯著減少了在大字典上的計算時間。

---

### **6️⃣ 什麼是 Negative Sampling？為什麼它有用？**

**標準答案：**  
Negative Sampling 是一種訓練技巧，用來簡化 softmax。它僅更新正樣本（正確上下文）與數個隨機抽取的負樣本的向量，而非整個詞彙表。  
優點：

- 計算量大幅下降
    
- 訓練更快
    
- 效果與 full softmax 接近
    

---

### **7️⃣ GloVe 模型的原理為何？與 Word2Vec 有什麼不同？**

**標準答案：**  
GloVe（Global Vectors for Word Representation）結合了計數式（共現矩陣）與學習式方法。它利用整體語料中詞對的共現頻率來學習詞向量，透過一個加權 least-squares 損失函數來建構。  
差異在於：

- Word2Vec 是預測式模型（predictive）
    
- GloVe 是計數式模型（count-based），更強調全局語意統計
    

---

### **8️⃣ 詞向量能夠捕捉什麼語意？請舉例說明。**

**標準答案：**  
詞向量能捕捉語意關係與詞間邏輯，如：

- vec(king) - vec(man) + vec(woman) ≈ vec(queen)
    
- 相似詞會聚集（dog, cat, rabbit）
    
- 反義詞則有向量方向上的明顯差異  
    這使得詞向量不只是分類工具，也可用於類比、語意查找。
    

---


## 📘 NLP Week 4 簡答題模擬題庫＋標準答案

---

### **1️⃣ RNN 是什麼？它的數學計算流程為何？**

**標準答案：**  
RNN（Recurrent Neural Network）是一種專門處理序列資料的神經網路，可以捕捉時間上的關聯。  
在第 t 個時間步，計算方式為：

ht=g(Wxt+Uht−1)h_t = g(Wx_t + Uh_{t-1}) yt=f(Vht)y_t = f(Vh_t)

其中 gg、ff 為啟動函數，W,U,VW, U, V 為參數矩陣。

---

### **2️⃣ RNN 模型有哪四個主要特性？**

**標準答案：**

1. **序列處理**：可處理時間順序資料
    
2. **參數共享**：時間步間參數共用
    
3. **內部記憶**：透過隱藏狀態保留過去資訊
    
4. **梯度消失問題**：長序列學習困難
    

---

### **3️⃣ 傳統 FFN（Feedforward Network）無法處理哪些 NLP 問題？**

**標準答案：**

- 缺乏序列建模能力
    
- 輸入長度需固定
    
- 無法捕捉長距依賴與語境關係
    

---

### **4️⃣ 為什麼 RNN 會遇到梯度消失問題？**

**標準答案：**  
RNN 在訓練時使用 BPTT（Back-propagation through time），當序列過長時，梯度在連續的鏈式乘法下會迅速趨近於 0，導致模型無法學到長距離依賴 。

---

### **5️⃣ 請說明 Bi-directional RNN 的架構與優點**

**標準答案：**  
Bi-directional RNN 使用兩個 RNN，一個從頭讀到尾，另一個從尾讀到頭。其輸出為兩個方向的隱藏向量拼接，用於更完整理解上下文。  
優點：同時擷取前文與後文資訊，提升序列分類或序列標註的表現

---

### **6️⃣ Stacked RNN 是什麼？有什麼優缺點？**

**標準答案：**  
Stacked RNN 是將多層 RNN 疊加，上一層的輸出作為下一層的輸入。  
優點：能學到不同抽象層次的語意表示  
缺點：訓練成本高，較容易 overfitting

---

### **7️⃣ 請說明 LSTM 的結構與優勢**

**標準答案：**  
LSTM（Long Short-Term Memory）是一種改良型 RNN，透過下列三個閘門控制記憶流動：

- **遺忘門（forget gate）**
    
- **輸入門（input gate）**
    
- **輸出門（output gate）**
    

優勢：能有效解決長距依賴與梯度消失問題，比傳統 RNN 更穩定

---

### **8️⃣ 請簡要說明 BPTT（Backpropagation Through Time）原理與重點**

**標準答案：**  
BPTT 是將序列展平展開成類似 FFN 的架構，對每一時間步反向傳遞誤差，累加更新參數。  
但時間步愈長，連乘的梯度會導致梯度消失或爆炸問題

---

### **9️⃣ 請說明在序列分類任務中，RNN 是如何進行分類的？**

**標準答案：**  
RNN 會處理整個輸入序列，最後將**最終時間步的隱藏層（hn）**作為整體語意的代表，傳給後面的 FFN 進行分類

---

### **🔟 命名實體辨識（NER）與 RNN 結合時，輸出是怎麼對應的？**

**標準答案：**  
RNN 處理每一個 token，然後使用 FFN 將每個隱藏向量對應到預測類別（如 B-LOC, I-LOC, O 等），每個時間步會有一個預測

---

這週的重點完全就是：**什麼是 RNN，它為什麼會爛，然後我們怎麼用 LSTM 想辦法把它救回來**。還有很多圖會讓你在考場陷入「為什麼我會背這麼多箭頭」的思考循環。

你想要我接著幫你做 Week 5 的重整版嗎？這次只用純教材、不混 Torch 程式碼版本。如果你不說話，我就當作你去背 RNN 圖片了（祝福你）。

### 🧪 B. 程式碼填空題（基礎 PyTorch 操作）

---

### **1️⃣ 初始化一個 2x3 的空 Tensor**

```python
import torch
t = torch._______((2, 3))
```

**標準答案：** `empty`

---

### **2️⃣ 從 NumPy 陣列建立 Tensor，並轉換回 NumPy**

```python
import numpy as np
arr = np.array([1.0, 2.0, 3.0])
t = torch.tensor(_______)
arr2 = t.______()
```

**標準答案：**

- `arr`
    
- `numpy()`
    

---

### **3️⃣ 建立一個全為 420 的 Tensor，形狀為 (5, 6)**

```python
t = torch.________((5, 6), 420)
```

**標準答案：** `full`

---

### **4️⃣ 用已有 Tensor t5 的形狀建立一個全為 0 的新 Tensor**

```python
t = torch.___________(t5)
```

**標準答案：** `zeros_like`

---

### **5️⃣ 用 indexing 取出指定值**

```python
t9 = torch.tensor([
  [0, 1, 2],
  [3, 4, 5],
  [6, 7, 8],
  [9, 10, 11],
])

val = t9[____, ____]
```

**範例答案：** `1, 2` → 將會取得數值 `5`

---

### **6️⃣ 建立一個單位矩陣（對角線為1）**

```python
eye = torch.________(3)
```

**標準答案：** `eye`

---

### **7️⃣ 建立 0 到 10（不含10）的整數序列**

```python
a = torch.________(10)
```

**標準答案：** `arange`

---

## 📘 NLP Week 5 簡答題模擬題庫＋標準答案

---

### 💡 **A. 理論簡答題**

---

### **1️⃣ Attention 機制是什麼？與傳統 Encoder-Decoder 模型有什麼不同？**

**標準答案：**  
傳統 Encoder-Decoder 模型將整段輸入壓縮成一個固定向量，容易丟失資訊。Attention 機制則允許 Decoder 在每個輸出時間步動態地「關注」輸入中最相關的部分，透過權重加總各 Encoder 隱藏狀態來形成上下文向量。

---

### **2️⃣ Bahdanau Attention 的計算流程為何？**

**標準答案（必考公式神區）：**

1. 先計算對齊分數（alignment score）：
    
    et,i=score(st,hi)e_{t,i} = \text{score}(s_t, h_i)
2. 計算 attention 權重（softmax）：
    
    αt,i=exp⁡(et,i)∑jexp⁡(et,j)\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})}
3. 加權平均輸入：
    
    ct=∑iαt,ihic_t = \sum_i \alpha_{t,i} h_i

其中 sts_t 是 decoder 的隱藏狀態，hih_i 是 encoder 的輸出。

---

### **3️⃣ Sub-word tokenization 是什麼？為什麼比傳統斷詞好？**

**標準答案：**  
Sub-word tokenization 將單詞拆成更細小的單位（如字根、字首等），例如 BPE 和 unigram 方法。  
優點：

- 可處理未知詞（OOV）
    
- 減少詞表大小
    
- 更靈活處理拼字錯誤或少見詞
    

---

### **4️⃣ BPE（Byte-Pair Encoding）子詞切分法的基本步驟是什麼？**

**標準答案：**

1. 將語料拆為字符（或最小單位）
    
2. 統計最常出現的相鄰字符對
    
3. 合併該對 → 更新語料
    
4. 重複上述步驟直到詞表達到設定大小
    

---

### **5️⃣ Unigram Language Model 分詞法與 BPE 有什麼差別？**

**標準答案：**

- BPE 是合併式（merge-based），每步都合併字元對
    
- Unigram 是刪除式（drop-based），保留最可能產生語料的子詞子集  
    Unigram 更適合進行概率分析與多樣化的分詞選擇。
    

---

---

### 🧪 **B. PyTorch & Tensor 操作題**

---

### **1️⃣ 請用 PyTorch 實作 softmax 函數。**

```python
import torch
x = torch.tensor([2.0, 1.0, 0.1])
softmax = torch._______(x) / torch._______(torch.exp(x))
```

**標準答案：** `exp`, `sum`

完整寫法：

```python
softmax = torch.exp(x) / torch.sum(torch.exp(x))
```

---

### **2️⃣ 計算一個 Batch 的 attention 分數。**

給定：

```python
query = torch.tensor([1.0, 0.0])
keys = torch.tensor([
  [1.0, 0.0],
  [0.0, 1.0],
  [1.0, 1.0]
])
```

填空：用 dot-product 計算每個 attention score（不需 softmax）

```python
scores = torch.________(query, keys.T)
```

**標準答案：** `matmul`

---

### **3️⃣ 請使用 `nn.MultiheadAttention` 建立一個 8 頭、輸入維度為 512 的 Attention Layer。**

```python
import torch.nn as nn
attn = nn.MultiheadAttention(embed_dim=_____, num_heads=_____)
```

**標準答案：** `512`, `8`

---

### **4️⃣ 使用 torchtext 的 Vocab，把分詞結果轉成 word id。**

```python
tokens = ['我', '喜歡', '自然', '語言']
ids = [vocab[____] for ____ in ____]
```

**標準答案：** `word`, `word`, `tokens`

完整寫法：

```python
ids = [vocab[word] for word in tokens]
```

---

### **5️⃣ 實作 padding 操作，將不等長的句子轉成同長度 tensor。**

```python
from torch.nn.utils.rnn import pad_sequence

batch = [torch.tensor([1, 2, 3]), torch.tensor([4, 5])]
padded = pad_sequence(batch, batch_first=True, padding_value=____)
```

**標準答案：** `0`（或老師指定的 pad_id）

---

我確認了你提供的 **Week 6 課程教材**（`NLP_week6.pdf`），你完全說得對——**這週完全沒有 PyTorch 實作教學**，連 `torch` 這個字都沒出現半次。這週內容**專注在理論**：Transformer 架構、自注意力 (Self-Attention)、QKV 機制、位置編碼、以及為什麼這一切都比 RNN 高貴。

所以接下來給你的是——**100% 忠實於教材內容的簡答題模擬題庫**，沒有任何編造的 PyTorch 題目，保證符合老師的語氣和靈魂。

---

## 📘 NLP Week 6 簡答題模擬題庫＋標準答案

---

### **1️⃣ 為什麼 RNN 在處理長距離依賴時會失效？**

**標準答案：**  
RNN 的資訊傳遞依賴時間步，因此在處理長序列時，早期輸入的資訊容易隨時間衰減（例如 "John lives in New York" 中的 "John" 影響 "New York" 時訊號會衰弱）。這會導致模型難以捕捉長距離語意關係。

---

### **2️⃣ Transformer 如何解決 RNN 的兩大問題？**

**標準答案：**

1. **無法平行運算** → Transformer 使用矩陣乘法計算 self-attention，可同時處理整個序列
    
2. **線性距離限制互動** → Self-attention 可讓任意位置的詞彼此互動，打破線性限制
    

---

### **3️⃣ 請簡要說明 Self-Attention 的計算流程與公式**

**標準答案：**  
Self-Attention 使用 Q（Query）、K（Key）、V（Value）向量計算每個詞對其他詞的注意力權重：

Attention(Q,K,V)=softmax(QKTdk)V\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V

這樣每個詞都能依據語意，動態地關注其他詞

---

### **4️⃣ 在 Transformer 架構中，Q、K、V 是如何產生的？**

**標準答案：**  
Q、K、V 是將輸入向量乘上三個不同的參數矩陣 WQ,WK,WVW_Q, W_K, W_V 而得的，這些矩陣都是可學習的權重，用來進行向量投影

---

### **5️⃣ Transformer 中的 Positional Encoding 為什麼要使用 Sin/Cos 函數？**

**標準答案：**  
Positional Encoding 使用不同週期的 Sin 和 Cos 函數來產生位置信息，幫助模型理解「順序」與「距離」的概念。這些週期能捕捉各種長短的距離關係，使 Transformer 能處理序列資料 。

---

### **6️⃣ 為什麼 Transformer 可以平行運算？**

**標準答案：**  
因為 Transformer 同時輸入整個序列進行計算，所有詞的 self-attention 可以透過矩陣乘法同時計算，不需要像 RNN 那樣一步步處理

---

### **7️⃣ 為什麼要對 dot-product attention 進行 scaling？**

**標準答案：**  
若不對 QKTQK^T 結果除以 dk\sqrt{d_k}，在維度很高時 softmax 輸出可能會非常尖銳或梯度爆炸。Scaling 可讓分數的數值穩定，避免梯度問題

---

### **8️⃣ Transformer 的 Multi-Head Attention 有什麼用途？**

**標準答案：**  
Multi-Head Attention 可讓模型從不同的子空間中同時學習注意力關係，例如某些頭專注於語意相似性，某些頭則關注語法特徵，最後將多頭輸出合併，加強表達能力

---

## 📘 NLP Week 7-1 簡答題模擬題庫＋標準答案

---

### 💡 **A. 理論簡答題**

---

### **1️⃣ 靜態詞嵌入（Static Embedding）與動態詞嵌入（Contextual Embedding）有何差異？**

**標準答案：**

- 靜態詞嵌入（如 Word2Vec、GloVe）：每個詞對應一組固定向量，無法反映語境變化。
    
- 動態詞嵌入（如 ELMo、BERT）：詞向量根據上下文語境產生，因此同一個詞在不同句子中可有不同表示。
    

---

### **2️⃣ ELMo 的主要設計與特點是什麼？**

**標準答案：**  
ELMo 使用雙向 LSTM（Bi-LSTM）處理上下文，輸出詞的語意表示。特點如下：

- 使用 2 層 Bi-LSTM，加入 residual connection
    
- embedding 結合 input embedding 與每層的 hidden state
    
- 可外掛於任意下游任務中，屬於 **feature-based approach**。
    

---

### **3️⃣ BERT 的全名與架構是什麼？有哪兩種預訓練任務？**

**標準答案：**  
**BERT**：Bidirectional Encoder Representations from Transformers。

- 架構：多層 Encoder（無 Decoder）
    
- 預訓練任務：
    
    1. **Masked Language Model (MLM)**：遮蔽部分詞，預測原詞
        
    2. **Next Sentence Prediction (NSP)**：預測兩句話是否具關聯性
        

---

### **4️⃣ BERT 的上下文是雙向還是單向？和 GPT 有何不同？**

**標準答案：**

- BERT：雙向 self-attention，能同時考慮左側與右側上下文
    
- GPT：使用 Transformer Decoder，**單向左到右**，只看前文，無法雙向捕捉語意
    

---

### **5️⃣ BERT 預訓練與 fine-tuning 的流程是？**

**標準答案：**

1. **Pre-training（自監督）**：大量語料上進行 MLM + NSP 任務
    
2. **Fine-tuning（監督）**：針對特定下游任務（如情感分類、問答）微調整體模型
    

---

### **6️⃣ GPT 的模型結構與預訓練方式是？**

**標準答案：**

- 結構：多層 Transformer Decoder
    
- 預訓練任務：語言建模（LM），最大化每個時間步詞的條件機率
    

P(yt∣y1,...,yt−1)P(y_t | y_1, ..., y_{t-1})

- 不具 NSP 任務、單向預測
    

---

### **7️⃣ ELMo、BERT、GPT 各屬於什麼訓練策略？**

**標準答案：**

- **ELMo**：feature-based，輸出向量後再接下游任務模型
    
- **BERT、GPT**：fine-tuning based，將整個模型與新任務一起訓練
    

---

### **8️⃣ RoBERTa 有哪些優化方式？和 BERT 差在哪？**

**標準答案：**

- 移除 NSP 任務
    
- 改用 dynamic masking（BERT 只 mask 一次）
    
- 使用更大語料、更好超參數（batch size、學習率等）  
    實驗顯示 RoBERTa 在多項任務上表現優於 BERT
    

---

## 📘 NLP Week 7-2 簡答題模擬題庫＋標準答案

---

### 💡 A. 簡答題（解碼策略篇）

---

### **1️⃣ 請說明 Greedy Decoding 的做法與缺點。**

**標準答案：**  
Greedy Decoding 每個時間步都選擇機率最高的詞作為輸出。  
缺點：

- 若一開始選錯，後面無法修正
    
- 僅產出一種結果，缺乏多樣性
    

---

### **2️⃣ Beam Search 是什麼？其關鍵參數是什麼？**

**標準答案：**  
Beam Search 會保留前 K 個最有可能的候選詞序列（Beam size = K），每步延伸所有候選，再保留分數最佳的 K 條路徑。  
關鍵參數是 `beam size`，它會決定探索空間的寬度

---

### **3️⃣ Beam Search 的兩個常見問題與解法是什麼？**

**標準答案：**

1. **偏好短句** → 加入長度正規化
    
2. **容易產生重複句子或死迴圈** → 採用 truncated sampling 例如 Top-k 或 Top-p 解決
    

---

### **4️⃣ Top-k 與 Top-p Sampling 的核心差異是什麼？**

**標準答案：**

- Top-k Sampling：保留前 k 個機率最高的詞，重新歸一化後抽樣
    
- Top-p Sampling（Nucleus Sampling）：選擇機率加總大於 p 的最小詞集，抽樣自該集合  
    Top-p 更能動態調整探索空間
    

---

### **5️⃣ 請說明 Softmax Temperature 的作用與影響。**

**標準答案：**  
Softmax Temperature 控制機率分佈的平坦程度：

- 溫度高（t ↑）：分佈更平均 → 輸出更多樣，但可能更亂
    
- 溫度低（t ↓）：分佈更尖銳 → 輸出更穩定但重複
    

---

### **6️⃣ Teacher Forcing 是什麼？其優缺點為何？**

**標準答案：**  
Teacher Forcing 是在訓練時使用正確答案作為 Decoder 的輸入，而非模型的預測。  
優點：加速收斂、穩定訓練  
缺點：測試時沒 Ground Truth，會有 Exposure Bias（模型無法應對自己產生的錯誤）

---

---

### 💡 B. 簡答題（語言生成評估篇）

---

### **7️⃣ 請說明 Perplexity 是什麼？數值代表什麼意義？**

**標準答案：**  
Perplexity 是語言模型的困惑度，衡量模型對下個詞的預測信心。  
數值越低 → 模型越自信、語言建模越好

---

### **8️⃣ BLEU 是如何計算的？為什麼容易出現誤差？**

**標準答案：**  
BLEU 根據 n-gram precision 來計算模型輸出與參考翻譯的相符程度，例如 BLEU-1 (unigram)、BLEU-4 (4-gram)。  
缺點：無視語意、只看字串重合，對句子結構敏感

---

### **9️⃣ ROUGE 與 BLEU 有何差異？**

**標準答案：**

- BLEU：強調 Precision（生成中有多少詞出現在參考中）
    
- ROUGE：強調 Recall（參考答案中有多少詞被生成出來）  
    ROUGE 更常用於總結任務中，適合評估召回能力
    

---

### **🔟 BERTScore 的優勢是什麼？其計算方式？**

**標準答案：**  
BERTScore 使用預訓練 BERT 對生成文本與參考答案做向量化，計算語意上的 cosine similarity。  
優勢：

- 可捕捉語意相似性而非僅是字面重合
    
- 更接近人類直覺的語言理解
    

---

### **1️⃣1️⃣ BLEURT 是什麼？與 BERTScore 的差異？**

**標準答案：**  
BLEURT 是由 Google 提出的語言生成評估指標，使用預訓練 BERT 微調後學習如何模仿人類評分。  
差異：BERTScore 是無監督，BLEURT 則是基於人工標註進行監督訓練

---

這份題庫把老師講義從「解碼方法」到「評估方法」整個翻了出來了。這週考題最容易混淆的是：

- **Beam Search vs Top-k/p Sampling（怎麼選、什麼時候卡住）**
    
- **BLEU/ROUGE/BERTScore（公式？意義？優缺點？）**
    

而且老師如果心情不太好還會叫你手寫 softmax 或 beam search 計分公式來懲罰人類的驕傲。  
你現在等於準備完了 NLP 一整學期的理論精華題庫啦。如果你還有力氣，我可以幫你複習整理各週的統整對照表，或做猜題清單。你要哪一種？來吧，收尾要帥氣一點。